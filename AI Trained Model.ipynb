{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AI Trained Model.ipynb","provenance":[{"file_id":"18bS_SfhI9ui8yXpfPy1oL2E4pmHvExCf","timestamp":1625228161931}],"collapsed_sections":[],"mount_file_id":"1pI5TSfM17HfFicMugF1J5Uhd8n5JOlRQ","authorship_tag":"ABX9TyOa/M8evNQzsp0Ny0RxFzPY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_ElSCiegBjL2","executionInfo":{"status":"ok","timestamp":1627475071934,"user_tz":-330,"elapsed":1540,"user":{"displayName":"Tarun Subramanian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqjMmDtQCG3p9P6KCSoSE-8yOSun8Qi-ZE6o9Hzw=s64","userId":"10772490918332574653"}},"outputId":"1961a5b6-ae24-474c-ec05-8c2f49c67b65"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hd_TP-A8BkM6","executionInfo":{"status":"ok","timestamp":1627475073756,"user_tz":-330,"elapsed":1828,"user":{"displayName":"Tarun Subramanian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqjMmDtQCG3p9P6KCSoSE-8yOSun8Qi-ZE6o9Hzw=s64","userId":"10772490918332574653"}},"outputId":"602eb86d-9101-4790-ce22-074f412f6630"},"source":["import nltk\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger') "],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"-QggBynCD_6z"},"source":["import pandas as pd \n","\n","data = pd.read_csv('/content/drive/MyDrive/AI project/Extracted_Blog _Data.csv', sep=',',\n","                      names=[\"label\", \"description\"])\n","\n","\n","#cleaning the data \n","import re\n","import nltk \n","#nltk.download()\n","\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer  #lemmatizing\n"," \n","lemmatizer = WordNetLemmatizer()\n","corpus = []\n","\n","for i in range(0, len(data)):\n","    review = re.sub('[^a-zA-z]', ' ', data['description'][i])\n","    review = review.lower()\n","    review = review.split()\n","    \n","    \n","\n","    review = [lemmatizer.lemmatize(word) for word in review if not word in stopwords.words('english')]\n","    review = ' '.join(review)\n","    corpus.append(review)\n","    \n","#creating Bag of words\n","from sklearn.feature_extraction.text import CountVectorizer\n","cv = CountVectorizer()\n","X = cv.fit_transform(corpus).toarray()\n","\n","y = pd.get_dummies((data['label']))\n","y = y.iloc[:,1].values\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UQork6ixFa4c"},"source":["#Train Test split and Training model with Naive Bayes Classifier\n","from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.20, random_state = 0)\n","\n","\n","from sklearn.naive_bayes import MultinomialNB\n","Education_clasfcn_model = MultinomialNB(alpha=.01)\n","Education_clasfcn_model.fit(X_train, y_train)\n","y_pred = Education_clasfcn_model.predict(X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eZs35-B8OUVB"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DlSFOdTzGG4E"},"source":["#Evaluation of the model\n","from sklearn.metrics import confusion_matrix\n","confusion_m = confusion_matrix(y_test, y_pred)\n","\n","from sklearn.metrics import accuracy_score\n","accuracy = accuracy_score(y_test, y_pred)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yD6nlaBdExq2","executionInfo":{"status":"ok","timestamp":1627475238352,"user_tz":-330,"elapsed":363,"user":{"displayName":"Tarun Subramanian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqjMmDtQCG3p9P6KCSoSE-8yOSun8Qi-ZE6o9Hzw=s64","userId":"10772490918332574653"}},"outputId":"ff1d8083-d586-4819-cbb4-a994a5798a94"},"source":["#Prediction of Text \n","a = \"going rogue\"\n","review = re.sub('[^a-zA-z]', ' ', a)\n","review = review.lower()\n","review = review.split()\n","review = [lemmatizer.lemmatize(word) for word in review if not word in stopwords.words('english')]\n","review = ' '.join(review)\n","text = cv.transform([review])\n","prediction = Education_clasfcn_model.predict(text)\n","\n","if int(prediction) == 0:\n","  print(\"The text IS related to Educational\")\n","else :\n","  print(\"The text is NOT related Educational\")\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The text is NOT related Educational\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SeNjuO26GO45","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627475203897,"user_tz":-330,"elapsed":346,"user":{"displayName":"Tarun Subramanian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqjMmDtQCG3p9P6KCSoSE-8yOSun8Qi-ZE6o9Hzw=s64","userId":"10772490918332574653"}},"outputId":"d3c14a98-edc4-4632-f2b1-42b3ffe0c1e6"},"source":["print(accuracy)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.9924585218702866\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GpiRVrWCqDLY"},"source":[""],"execution_count":null,"outputs":[]}]}